{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/OQtEdTjDEVENe0WKoz7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sh1nysparkly/relevance-validation/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kImcrsSAHQkU"
      },
      "outputs": [],
      "source": [
        "# --- Imports & config ---\n",
        "import os, time, re, json, math\n",
        "import pandas as pd\n",
        "from slugify import slugify\n",
        "\n",
        "# Optional token counts (nice-to-have; fine if it fails)\n",
        "try:\n",
        "    import tiktoken\n",
        "    _enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    def token_count(text: str) -> int:\n",
        "        return len(_enc.encode(text or \"\"))\n",
        "except Exception:\n",
        "    def token_count(text: str) -> int:\n",
        "        return 0\n",
        "\n",
        "# Google Cloud NL\n",
        "from google.cloud import language_v1\n",
        "\n",
        "MAX_CLASSIFY_CHARS = 9000\n",
        "MAX_ENTITY_CHARS   = 120000\n",
        "SLEEP_BETWEEN_CALLS = 0.1\n",
        "RETRIES = 3\n",
        "TIMEOUT = 30\n",
        "\n",
        "# --- Helper funcs (keep in this cell) ---\n",
        "def std_col(s: str) -> str:\n",
        "    return re.sub(r'[^a-z0-9]+', '_', (s or '').strip().lower()).strip('_')\n",
        "\n",
        "def best_text_column(df: pd.DataFrame) -> str:\n",
        "    candidates = [\"text\",\"copy\",\"content\",\"body\"]\n",
        "    cols = [c for c in df.columns if std_col(c) in candidates]\n",
        "    if not cols:\n",
        "        raise ValueError(\"No text column found. Add a 'text' column (or 'copy'/'content'/'body').\")\n",
        "    return cols[0]\n",
        "\n",
        "def safe_get(row: pd.Series, *names: str) -> str:\n",
        "    for n in names:\n",
        "        if n and (n in row) and pd.notna(row[n]):\n",
        "            return str(row[n])\n",
        "    return \"\"\n",
        "\n",
        "def combine_text(row: pd.Series, title_col, meta_col, text_col):\n",
        "    title = safe_get(row, title_col) if title_col else \"\"\n",
        "    meta  = safe_get(row, meta_col)  if meta_col  else \"\"\n",
        "    text  = safe_get(row, text_col)\n",
        "\n",
        "    parts = []\n",
        "    if title: parts.append(f\"Title: {title}\")\n",
        "    if meta:  parts.append(f\"Meta Description: {meta}\")\n",
        "    if text:  parts.append(text)\n",
        "\n",
        "    combined = \"\\n\".join(parts).strip()\n",
        "    return combined, text\n",
        "\n",
        "def mk_document(text: str) -> language_v1.Document:\n",
        "    return language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT, language=\"en\")\n",
        "\n",
        "def with_backoff(fn, *args, **kwargs):\n",
        "    delay = 1.0\n",
        "    last_exc = None\n",
        "    for _ in range(RETRIES):\n",
        "        try:\n",
        "            return fn(*args, timeout=TIMEOUT, **kwargs)\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            time.sleep(delay); delay *= 2\n",
        "    raise last_exc\n",
        "\n",
        "def analyze_entities(client: language_v1.LanguageServiceClient, text: str):\n",
        "    if not text: return []\n",
        "    text = text[:MAX_ENTITY_CHARS]\n",
        "    resp = with_backoff(client.analyze_entities, document=mk_document(text), encoding_type=language_v1.EncodingType.UTF8)\n",
        "    out = []\n",
        "    for ent in resp.entities:\n",
        "        out.append({\n",
        "            \"name\": ent.name,\n",
        "            \"type\": language_v1.Entity.Type(ent.type_).name,\n",
        "            \"salience\": getattr(ent, \"salience\", 0.0),\n",
        "            \"wikipedia_url\": ent.metadata.get(\"wikipedia_url\",\"\"),\n",
        "            \"mid\": ent.metadata.get(\"mid\",\"\"),\n",
        "            \"mentions\": len(ent.mentions),\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def classify_text(client: language_v1.LanguageServiceClient, text: str):\n",
        "    if not text: return []\n",
        "    text = text[:MAX_CLASSIFY_CHARS]\n",
        "    try:\n",
        "        resp = with_backoff(client.classify_text, document=mk_document(text))\n",
        "    except Exception:\n",
        "        return []\n",
        "    return [{\"category\": c.name, \"confidence\": c.confidence} for c in resp.categories]\n",
        "\n",
        "def topn_entities(ents, n=5):\n",
        "    if not ents: return \"\"\n",
        "    top = sorted(ents, key=lambda e: e.get(\"salience\",0), reverse=True)[:n]\n",
        "    return \"; \".join([f\"{e['name']} ({e.get('salience',0):.2f})\" for e in top])\n",
        "\n",
        "def topn_categories(cats, n=3):\n",
        "    if not cats: return \"\"\n",
        "    top = sorted(cats, key=lambda c: c.get(\"confidence\",0), reverse=True)[:n]\n",
        "    return \"; \".join([f\"{c['category']} ({c.get('confidence',0):.2f})\" for c in top])\n",
        "\n",
        "# --- The ONE BIG function: keep together in this cell ---\n",
        "def run(input_csv: str, output_dir: str = \".\"):\n",
        "    df = pd.read_csv(input_csv)\n",
        "    rename_map = {c: std_col(c) for c in df.columns}\n",
        "    df = df.rename(columns=rename_map)\n",
        "\n",
        "    text_col = best_text_column(df)\n",
        "    slug_col = \"slug\" if \"slug\" in df.columns else None\n",
        "    title_col = \"page_title\" if \"page_title\" in df.columns else None\n",
        "    meta_col  = \"meta_description\" if \"meta_description\" in df.columns else None\n",
        "\n",
        "    # page IDs\n",
        "    ids = []\n",
        "    for i, row in df.iterrows():\n",
        "        if slug_col and str(row.get(slug_col,\"\")).strip():\n",
        "            base = str(row[slug_col]).strip()\n",
        "        elif title_col and str(row.get(title_col,\"\")).strip():\n",
        "            base = slugify(str(row[title_col]).strip())\n",
        "        else:\n",
        "            base = f\"page-{i+1}\"\n",
        "        ids.append(base)\n",
        "    df[\"__page_id__\"] = ids\n",
        "\n",
        "    # combined and raw\n",
        "    combined_texts, raw_texts = [], []\n",
        "    for _, row in df.iterrows():\n",
        "        combined, raw = combine_text(row, title_col, meta_col, text_col)\n",
        "        combined_texts.append(combined)\n",
        "        raw_texts.append(raw)\n",
        "    df[\"__combined__\"] = combined_texts\n",
        "    df[\"__raw__\"] = raw_texts\n",
        "\n",
        "    client = language_v1.LanguageServiceClient()\n",
        "\n",
        "    ent_rows, cat_rows, summary_rows = [], [], []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        page_id = row[\"__page_id__\"]\n",
        "        title   = row.get(title_col, \"\") if title_col else \"\"\n",
        "        combined = row[\"__combined__\"]\n",
        "        raw      = row[\"__raw__\"]\n",
        "\n",
        "        # Entities (use full text if present; else combined)\n",
        "        entities = analyze_entities(client, raw if raw else combined)\n",
        "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "        # Categories (use combined: title + meta + copy)\n",
        "        categories = classify_text(client, combined)\n",
        "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "\n",
        "        for e in entities:\n",
        "            ent_rows.append({\n",
        "                \"page_id\": page_id,\n",
        "                \"page_title\": title,\n",
        "                \"entity\": e[\"name\"],\n",
        "                \"type\": e[\"type\"],\n",
        "                \"salience\": e[\"salience\"],\n",
        "                \"mentions\": e[\"mentions\"],\n",
        "                \"wikipedia_url\": e[\"wikipedia_url\"],\n",
        "                \"mid\": e[\"mid\"],\n",
        "            })\n",
        "\n",
        "        for c in categories:\n",
        "            cat_rows.append({\n",
        "                \"page_id\": page_id,\n",
        "                \"page_title\": title,\n",
        "                \"category\": c[\"category\"],\n",
        "                \"confidence\": c[\"confidence\"],\n",
        "            })\n",
        "\n",
        "        summary_rows.append({\n",
        "            \"page_id\": page_id,\n",
        "            \"page_title\": title,\n",
        "            \"chars_text\": len((row.get(text_col) or \"\")),\n",
        "            \"chars_combined\": len(combined),\n",
        "            \"tokens_text\": token_count(row.get(text_col) or \"\"),\n",
        "            \"tokens_combined\": token_count(combined),\n",
        "            \"entity_count\": len(entities),\n",
        "            \"category_count\": len(categories),\n",
        "            \"top_entities\": topn_entities(entities, n=5),\n",
        "            \"top_categories\": topn_categories(categories, n=3),\n",
        "        })\n",
        "\n",
        "    entities_df = pd.DataFrame(ent_rows)\n",
        "    categories_df = pd.DataFrame(cat_rows)\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "\n",
        "    if not entities_df.empty:\n",
        "        entities_df = entities_df.sort_values([\"page_id\",\"salience\"], ascending=[True, False])\n",
        "    if not categories_df.empty:\n",
        "        categories_df = categories_df.sort_values([\"page_id\",\"confidence\"], ascending=[True, False])\n",
        "    summary_df = summary_df.sort_values([\"page_id\"])\n",
        "\n",
        "    output_dir = os.path.abspath(output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    ent_path = os.path.join(output_dir, \"entities.csv\")\n",
        "    cat_path = os.path.join(output_dir, \"categories.csv\")\n",
        "    sum_path = os.path.join(output_dir, \"pages_summary.csv\")\n",
        "\n",
        "    entities_df.to_csv(ent_path, index=False)\n",
        "    categories_df.to_csv(cat_path, index=False)\n",
        "    summary_df.to_csv(sum_path, index=False)\n",
        "\n",
        "    print(f\"\\nDone.\\nWrote:\\n - {ent_path}\\n - {cat_path}\\n - {sum_path}\")\n",
        "    return ent_path, cat_path, sum_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you used files.upload() earlier:\n",
        "# input_csv = \"your_uploaded_file.csv\"\n",
        "\n",
        "# Choose an output folder\n",
        "out_dir = \"out\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "run(input_csv, out_dir)"
      ],
      "metadata": {
        "id": "A20oprXHHZUg",
        "outputId": "6fdf9964-277e-41e4-a5aa-4e61082f93e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'input_csv' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3560955874.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'input_csv' is not defined"
          ]
        }
      ]
    }
  ]
}