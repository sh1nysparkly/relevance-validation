# -*- coding: utf-8 -*-
"""CONTENT BRIEF KEYWORD CLUSTERING TOOL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12EcNuGnvSgkJQuh4mhaNrt4-kgb9R13l
"""

"""
CONTENT BRIEF KEYWORD CLUSTERING TOOL
=====================================
This notebook helps you cluster keywords, assign them to pages, and identify cannibalization risks.

SETUP:
1. Upload your keyword CSV (the 20K+ row one)
2. Make sure it has columns: 'keyword', 'volume' (and optionally 'intent', 'audience')
3. Run each cell in order
4. Review outputs and adjust thresholds as needed

"""

# ============================================================================
# STEP 1: INSTALL & IMPORT
# ============================================================================

!pip install openai scikit-learn pandas numpy -q

import pandas as pd
import numpy as np
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN, AgglomerativeClustering
import json
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# Initialize OpenAI client
# You'll need to set your API key - either set it in Colab secrets or paste here
from google.colab import userdata
client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))

print("‚úÖ Libraries loaded")

# ============================================================================
# STEP 2: LOAD YOUR DATA
# ============================================================================

# Upload your keyword CSV
from google.colab import files
uploaded = files.upload()

# Load the CSV (adjust filename as needed)
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

print(f"‚úÖ Loaded {len(df)} keywords")
print(f"Columns: {list(df.columns)}")
print("\nFirst few rows:")
print(df.head())

# ============================================================================
# STEP 3: DEFINE YOUR URL STRUCTURE
# ============================================================================

# Your pages - organized by parent section
URL_STRUCTURE = {
    'things-to-do': [
        'couples', 'golf-sports', 'seasonal', 'family', 'wine-food',
        'luxury', 'train', 'friends', 'accessible',
        'seniors', 'walking-tours', 'boat', 'bike'
    ],
    'vacation-packages': [
        'adventure', 'beach', 'family',
        'long-stay', 'food-wine', 'golf-vacations-sports',
        'bachelor-bachelorette', 'luxury'
    ]
}

# Flatten for easier lookup
ALL_PAGES = []
for parent, pages in URL_STRUCTURE.items():
    for page in pages:
        ALL_PAGES.append(f"{parent}/{page}")

print(f"‚úÖ Tracking {len(ALL_PAGES)} pages")

# ============================================================================
# STEP 4: FILTER & PREP KEYWORDS
# ============================================================================

# Clean up the data
df = df.dropna(subset=['keyword'])
df['keyword'] = df['keyword'].str.strip().str.lower()
df = df.drop_duplicates(subset=['keyword'])

# If you have volume column, make sure it's numeric
if 'volume' in df.columns:
    df['volume'] = pd.to_numeric(df['volume'], errors='coerce').fillna(0)
else:
    df['volume'] = 0

print(f"‚úÖ Cleaned data: {len(df)} unique keywords")

# Optional: Filter by volume threshold
MIN_VOLUME = 10  # Adjust as needed
df_filtered = df[df['volume'] >= MIN_VOLUME].copy()
print(f"‚úÖ After volume filter (>={MIN_VOLUME}): {len(df_filtered)} keywords")

# ============================================================================
# STEP 5: GENERATE EMBEDDINGS
# ============================================================================

def get_embedding(text, model="text-embedding-3-small"):
    """Get embedding for a single text string"""
    text = text.replace("\n", " ")
    response = client.embeddings.create(input=[text], model=model)
    return response.data[0].embedding

def get_embeddings_batch(texts, batch_size=100):
    """Get embeddings for multiple texts in batches"""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}...")
        batch_embeddings = [get_embedding(text) for text in batch]
        embeddings.extend(batch_embeddings)
    return embeddings

# Generate embeddings (this will take a few minutes for large datasets)
print("üîÑ Generating embeddings... (this may take a while)")
keywords_list = df_filtered['keyword'].tolist()
embeddings = get_embeddings_batch(keywords_list)
df_filtered['embedding'] = embeddings

print(f"‚úÖ Generated embeddings for {len(embeddings)} keywords")

# ============================================================================
# STEP 6: CLUSTER KEYWORDS
# ============================================================================

# Reset index to match embedding matrix positions
df_filtered = df_filtered.reset_index(drop=True)
print("‚úÖ Index reset to match embedding positions")

# Convert embeddings to numpy array
embedding_matrix = np.array(df_filtered['embedding'].tolist())

# Calculate similarity matrix
similarity_matrix = cosine_similarity(embedding_matrix)

# Cluster using Agglomerative Clustering
# distance_threshold controls cluster tightness (lower = tighter clusters)
clustering = AgglomerativeClustering(
    n_clusters=None,
    distance_threshold=0.5,  # Adjust: 0.3-0.4 = tight, 0.5-0.7 = loose
    metric='cosine',
    linkage='average'
)

df_filtered['cluster'] = clustering.fit_predict(embedding_matrix)

print(f"‚úÖ Created {df_filtered['cluster'].nunique()} clusters")

# ============================================================================
# STEP 7: ANALYZE CLUSTERS
# ============================================================================

def analyze_cluster(cluster_df, embedding_matrix):
    """Analyze a single cluster and assign keyword priorities"""
    if len(cluster_df) == 1:
        return {
            'primary': cluster_df.iloc[0]['keyword'],
            'secondary': [],
            'tertiary': [],
            'coherence': 1.0,
            'keywords': cluster_df.to_dict('records')
        }

    # Get embeddings for this cluster
    cluster_embeddings = embedding_matrix[cluster_df.index]

    # Calculate centroid
    centroid = cluster_embeddings.mean(axis=0).reshape(1, -1)

    # Calculate distance from centroid for each keyword
    distances = cosine_similarity(cluster_embeddings, centroid).flatten()
    cluster_df = cluster_df.copy()
    cluster_df['centrality'] = distances

    # Calculate coherence (average pairwise similarity)
    pairwise_sim = cosine_similarity(cluster_embeddings)
    coherence = (pairwise_sim.sum() - len(cluster_df)) / (len(cluster_df) * (len(cluster_df) - 1))

    # Sort by: centrality first, then volume
    cluster_df = cluster_df.sort_values(['centrality', 'volume'], ascending=[False, False])

    # Assign tiers
    n = len(cluster_df)
    primary_idx = 0
    secondary_idx = min(3, n)

    return {
        'primary': cluster_df.iloc[primary_idx]['keyword'],
        'secondary': cluster_df.iloc[1:secondary_idx]['keyword'].tolist(),
        'tertiary': cluster_df.iloc[secondary_idx:]['keyword'].tolist() if n > secondary_idx else [],
        'coherence': float(coherence),
        'keywords': cluster_df.to_dict('records')
    }

# Analyze all clusters
cluster_analysis = {}
for cluster_id in df_filtered['cluster'].unique():
    cluster_df = df_filtered[df_filtered['cluster'] == cluster_id]
    cluster_analysis[cluster_id] = analyze_cluster(cluster_df, embedding_matrix)

print(f"‚úÖ Analyzed {len(cluster_analysis)} clusters")

# ============================================================================
# STEP 7.5: EXPORT CLEAN CLUSTER DATA
# ============================================================================

print("\n" + "="*80)
print("üìä EXPORTING DETAILED CLUSTER DATA")
print("="*80)

# Create a clean export with all keywords and their cluster assignments
cluster_export = []

for cluster_id in sorted(df_filtered['cluster'].unique()):
    cluster_df = df_filtered[df_filtered['cluster'] == cluster_id]

    if cluster_id == -1:
        # Noise points / unclustered keywords
        cluster_name = "Unclustered"
        for idx, row in cluster_df.iterrows():
            cluster_export.append({
                'cluster_id': 'Unclustered',
                'cluster_name': 'Unclustered',
                'keyword': row['keyword'],
                'volume': row['volume'],
                'centrality': 0,  # No centrality for noise
                'cluster_size': len(cluster_df),
                'cluster_coherence': 0
            })
    else:
        # Get cluster analysis data
        analysis = cluster_analysis.get(cluster_id, {})
        cluster_name = analysis.get('primary', f'Cluster {cluster_id}')
        coherence = analysis.get('coherence', 0)

        # Get centrality scores if available
        cluster_embeddings = np.array([df_filtered.loc[idx, 'embedding'] for idx in cluster_df.index])

        if len(cluster_embeddings) > 1:
            centroid = cluster_embeddings.mean(axis=0).reshape(1, -1)
            centralities = cosine_similarity(cluster_embeddings, centroid).flatten()
        else:
            centralities = [1.0]

        # Add each keyword in this cluster
        for i, (idx, row) in enumerate(cluster_df.iterrows()):
            cluster_export.append({
                'cluster_id': cluster_id,
                'cluster_name': cluster_name,
                'keyword': row['keyword'],
                'volume': row['volume'],
                'centrality': centralities[i] if i < len(centralities) else 0,
                'cluster_size': len(cluster_df),
                'cluster_coherence': coherence
            })

# Create DataFrame
cluster_export_df = pd.DataFrame(cluster_export)

# Sort: by cluster_id, then by centrality (highest first), then by volume
cluster_export_df = cluster_export_df.sort_values(
    ['cluster_id', 'centrality', 'volume'],
    ascending=[True, False, False]
)

# Reorder columns for usability
cluster_export_df = cluster_export_df[[
    'cluster_id',
    'cluster_name',
    'keyword',
    'volume',
    'centrality',
    'cluster_size',
    'cluster_coherence'
]]

# Export
cluster_export_df.to_csv('clusters_detailed.csv', index=False)

print(f"‚úÖ Exported clusters_detailed.csv")
print(f"   Total keywords: {len(cluster_export_df)}")
print(f"   Clustered: {len(cluster_export_df[cluster_export_df['cluster_id'] != 'Unclustered'])}")
print(f"   Unclustered: {len(cluster_export_df[cluster_export_df['cluster_id'] == 'Unclustered'])}")
print(f"   Total clusters: {cluster_export_df['cluster_id'].nunique() - 1}")  # -1 for unclustered

# Show sample
print("\nüìã Sample of detailed clusters export:")
print(cluster_export_df.head(20).to_string(index=False))

# ============================================================================
# STEP 8: MAP CLUSTERS TO PAGES (MANUAL REVIEW NEEDED)
# ============================================================================

# This part requires your manual review!
# We'll show you the clusters and you assign them to pages

print("\n" + "="*80)
print("CLUSTER REVIEW - Assign clusters to your pages")
print("="*80)

# Sort clusters by size and coherence
cluster_summary = []
for cluster_id, analysis in cluster_analysis.items():
    cluster_summary.append({
        'cluster_id': cluster_id,
        'size': len(analysis['keywords']),
        'coherence': analysis['coherence'],
        'primary': analysis['primary'],
        'secondary': ', '.join(analysis['secondary'][:3])
    })

cluster_summary_df = pd.DataFrame(cluster_summary)
cluster_summary_df = cluster_summary_df.sort_values(['coherence', 'size'], ascending=[False, False])

print("\nTop 20 clusters (review these first):")
print(cluster_summary_df.head(20).to_string(index=False))

# Export for manual review
cluster_summary_df.to_csv('cluster_summary.csv', index=False)
print("\n‚úÖ Exported cluster_summary.csv for review")

# ============================================================================
# STEP 9: DETECT CANNIBALIZATION RISK
# ============================================================================

def calculate_page_overlap(page1_keywords, page2_keywords, embedding_matrix):
    """Calculate semantic overlap between two pages' keyword sets"""
    if len(page1_keywords) == 0 or len(page2_keywords) == 0:
        return 0.0

    # Get embeddings
    emb1 = embedding_matrix[page1_keywords]
    emb2 = embedding_matrix[page2_keywords]

    # Calculate cross-similarity
    cross_sim = cosine_similarity(emb1, emb2)

    # Return average of top similarities
    return cross_sim.max(axis=1).mean()

# For now, let's identify HIGH RISK page pairs based on URL similarity
HIGH_RISK_PAIRS = [
    ('things-to-do/golf-sports', 'vacation-packages/golf-vacations-sports'),
    ('things-to-do/wine-food', 'vacation-packages/food-wine'),
    ('things-to-do/family', 'vacation-packages/family'),
    ('things-to-do/luxury', 'vacation-packages/luxury'),
    ('things-to-do/train', 'vacation-packages/train-travel'),
    ('things-to-do/pet-friendly', 'vacation-packages/pet-friendly-travel'),
]

print("\n" + "="*80)
print("‚ö†Ô∏è  HIGH RISK CANNIBALIZATION PAIRS")
print("="*80)
print("\nThese page pairs have similar topics and may compete:")
for page1, page2 in HIGH_RISK_PAIRS:
    print(f"\nüî¥ {page1}")
    print(f"   vs")
    print(f"üî¥ {page2}")
    print(f"   ‚Üí Recommendation: Enforce intent distinction")
    print(f"      - {page1}: Informational (guides, ideas, tips)")
    print(f"      - {page2}: Commercial (packages, bookings, deals)")

# ============================================================================
# STEP 10: AUTOMATED KEYWORD-TO-PAGE MATCHING
# ============================================================================

print("\n" + "="*80)
print("ü§ñ AUTOMATED KEYWORD MATCHING")
print("="*80)

# Define semantic profiles for each page
# For each page, provide 3-5 keywords/phrases that describe what it should be about
PAGE_PROFILES = {
    'things-to-do/couples': [
        'romantic activities couples',
        'things to do couples',
        'romantic date ideas',
        'couples experiences',
        'romantic getaway activities'
    ],
    'things-to-do/golf-sports': [
        'golf courses',
        'sports activities',
        'things to do golf',
        'where to play golf',
        'sports experiences'
    ],
'vacation-packages/bachelor-bachelorette': [
    'bachelorette party',
    'bachelor party ideas',
    'bachelorette destinations',
    'bachelorette party ideas',
    'bachelor party destinations',
    'bachelorette ideas',
    'bachelorette trip'
],
    'things-to-do/family': [
        'family activities',
        'things to do with kids',
        'family friendly attractions',
        'kid friendly activities',
        'family tours',
        'family friendly experiences',
        'family fun'
    ],
    'things-to-do/wine-food': [
        'food tours',
        'wine tasting',
        'culinary experiences',
        'restaurants',
        'dining experiences'
    ],
    'things-to-do/luxury': [
        'luxury experiences',
        'upscale activities',
        'high end things to do',
        'premium experiences',
        'exclusive activities'
    ],
    'things-to-do/train': [
        'train journeys',
        'scenic train rides',
        'rail experiences',
        'train travel',
        'railway attractions'
    ],
    'things-to-do/friends': [
        'group activities',
        'things to do with friends',
        'fun activities groups',
        'friend getaway ideas',
        'girls trip',
        'things to do with friends',
        'girls trip destinations',
        'places to go with friends',
        'friend trip',
        'girls weekend getaways',
        'group vacation ideas',
        'fun places to travel with friends'
    ],
    'things-to-do/accessible': [
        'accessible activities',
        'wheelchair accessible',
        'disability friendly',
        'accessible attractions',
        'mobility assistance'
    ],
    'things-to-do/seniors': [
        'senior activities',
        'things to do seniors',
        'retirement travel',
        'senior friendly attractions',
        'mature traveler'
    ],
    'things-to-do/walking-tours': [
        'walking tours',
        'guided walks',
        'city walking tours',
        'walking experiences',
        'pedestrian tours'
    ],
    'things-to-do/boat': [
        'boat tours',
        'sailing experiences',
        'water activities',
        'cruises',
        'maritime experiences'
    ],
    'things-to-do/bike': [
        'bike tours',
        'cycling routes',
        'bicycle rentals',
        'biking experiences',
        'cycling adventures'
    ],
   'vacation-packages/couples': [
    'couples resort',
    'couples resorts',
    'all inclusive for couples',
    'vacation for couples',
    'vacation ideas for couples',
    'couple traveling',
    'couples travelling',
    'cruise for couples'
],
    'vacation-packages/adventure': [
        'adventure vacation packages',
        'adventure travel deals',
        'outdoor adventure packages',
        'adventure getaway packages',
        'adventure vacation deals'
    ],
    'vacation-packages/beach': [
        'beach vacation packages',
        'beach resort packages',
        'beach getaway deals',
        'all inclusive beach packages',
        'beach vacation deals'
    ],
    'vacation-packages/family': [
        'family vacation packages',
        'family getaway deals',
        'all inclusive family packages',
        'family vacation deals',
        'family package deals'
    ],
    'vacation-packages/long-stay': [
        'extended stay packages',
        'long term vacation packages',
        'monthly vacation packages',
        'extended vacation deals',
        'long stay vacation'
    ]
}

print("üìù Defined semantic profiles for all 24 pages")
print("\nüîÑ Generating embeddings for page profiles...")

# Generate embeddings for page profiles
page_profile_embeddings = {}
for page, profile_keywords in PAGE_PROFILES.items():
    # Combine profile keywords into a single text
    profile_text = ' '.join(profile_keywords)
    page_profile_embeddings[page] = get_embedding(profile_text)

print(f"‚úÖ Generated profile embeddings for {len(page_profile_embeddings)} pages")

# ============================================================================
# STEP 11: CALCULATE KEYWORD-TO-PAGE SIMILARITY & AUTO-ASSIGN
# ============================================================================

print("\n" + "="*80)
print("üéØ MATCHING KEYWORDS TO PAGES")
print("="*80)

# Calculate similarity between each keyword and each page profile
print("\nüîÑ Calculating semantic similarity...")

# Prepare matrices
keyword_embeddings = np.array(df_filtered['embedding'].tolist())
page_names = list(page_profile_embeddings.keys())
page_embeddings = np.array([page_profile_embeddings[page] for page in page_names])

# Calculate similarity matrix (keywords x pages)
similarity_matrix = cosine_similarity(keyword_embeddings, page_embeddings)

# For each keyword, find best matching page
df_filtered['best_match_page'] = ''
df_filtered['best_match_score'] = 0.0
df_filtered['second_best_page'] = ''
df_filtered['second_best_score'] = 0.0

SIMILARITY_THRESHOLD = 0.65  # Adjust: higher = stricter matching (0.6-0.7 is good range)

for i, row in df_filtered.iterrows():
    # Get similarity scores for this keyword to all pages
    scores = similarity_matrix[i]

    # Find top 2 matches
    top_2_indices = np.argsort(scores)[-2:][::-1]

    best_idx = top_2_indices[0]
    best_score = scores[best_idx]

    if best_score >= SIMILARITY_THRESHOLD:
        df_filtered.at[i, 'best_match_page'] = page_names[best_idx]
        df_filtered.at[i, 'best_match_score'] = best_score

        if len(top_2_indices) > 1:
            second_idx = top_2_indices[1]
            df_filtered.at[i, 'second_best_page'] = page_names[second_idx]
            df_filtered.at[i, 'second_best_score'] = scores[second_idx]

# Count assignments
matched_keywords = df_filtered[df_filtered['best_match_page'] != '']
unmatched_keywords = df_filtered[df_filtered['best_match_page'] == '']

print(f"\n‚úÖ Matching complete!")
print(f"   üìå Matched: {len(matched_keywords)} keywords ({len(matched_keywords)/len(df_filtered)*100:.1f}%)")
print(f"   ‚ùì Unmatched: {len(unmatched_keywords)} keywords ({len(unmatched_keywords)/len(df_filtered)*100:.1f}%)")

# Show distribution across pages
print(f"\nüìä Keywords per page:")
page_counts = matched_keywords['best_match_page'].value_counts()
for page, count in page_counts.items():
    total_volume = matched_keywords[matched_keywords['best_match_page'] == page]['volume'].sum()
    print(f"   {page}: {count} keywords ({total_volume:,} total volume)")

# Export results
matched_keywords.to_csv('matched_keywords.csv', index=False)
unmatched_keywords.to_csv('unmatched_keywords.csv', index=False)
print(f"\n‚úÖ Exported matched_keywords.csv and unmatched_keywords.csv")

# ============================================================================
# STEP 12: REVIEW AMBIGUOUS MATCHES (Optional)
# ============================================================================

print("\n" + "="*80)
print("‚ö†Ô∏è  AMBIGUOUS MATCHES - Review These")
print("="*80)

# Find keywords where top 2 matches are very close
AMBIGUITY_THRESHOLD = 0.05  # If top 2 scores are within 0.05, flag as ambiguous

ambiguous = matched_keywords[
    (matched_keywords['second_best_score'] > 0) &
    (matched_keywords['best_match_score'] - matched_keywords['second_best_score'] < AMBIGUITY_THRESHOLD)
].copy()

if len(ambiguous) > 0:
    print(f"\n‚ö†Ô∏è  Found {len(ambiguous)} ambiguous keywords (top 2 pages are similar matches):\n")

    # Show top 20 most ambiguous
    ambiguous = ambiguous.sort_values('volume', ascending=False)
    for _, row in ambiguous.head(20).iterrows():
        print(f"'{row['keyword']}' (vol: {row['volume']:.0f})")
        print(f"   1st: {row['best_match_page']} ({row['best_match_score']:.3f})")
        print(f"   2nd: {row['second_best_page']} ({row['second_best_score']:.3f})")
        print(f"   Difference: {row['best_match_score'] - row['second_best_score']:.3f}\n")

    ambiguous.to_csv('ambiguous_keywords.csv', index=False)
    print(f"‚úÖ Exported ambiguous_keywords.csv - Review these manually if needed")
else:
    print("‚úÖ No highly ambiguous matches found")

# ============================================================================
# STEP 13: GENERATE CONTENT BRIEFS
# ============================================================================

print("\n" + "="*80)
print("üìù GENERATING CONTENT BRIEFS")
print("="*80)

def generate_briefs_from_matches(matched_df):
    """Generate content briefs based on auto-matched keywords"""

    briefs = {}

    for page in PAGE_PROFILES.keys():
        page_keywords = matched_df[matched_df['best_match_page'] == page].copy()

        if len(page_keywords) == 0:
            continue

        # Sort by match score first, then volume
        page_keywords = page_keywords.sort_values(['best_match_score', 'volume'],
                                                   ascending=[False, False])

        # Assign tiers
        n = len(page_keywords)
        primary_n = min(2, n)  # Top 2 as primary
        secondary_n = min(8, n)  # Next 6 as secondary

        briefs[page] = {
            'primary': page_keywords.head(primary_n)[['keyword', 'volume', 'best_match_score']].to_dict('records'),
            'secondary': page_keywords.iloc[primary_n:secondary_n][['keyword', 'volume', 'best_match_score']].to_dict('records'),
            'tertiary': page_keywords.iloc[secondary_n:30][['keyword', 'volume', 'best_match_score']].to_dict('records'),
            'total_keywords': len(page_keywords),
            'total_volume': page_keywords['volume'].sum(),
            'avg_match_score': page_keywords['best_match_score'].mean()
        }

    return briefs

# Generate briefs
briefs = generate_briefs_from_matches(matched_keywords)

print(f"\n‚úÖ Generated briefs for {len(briefs)} pages\n")

# Display briefs
for page, brief in sorted(briefs.items()):
    print(f"\n{'='*80}")
    print(f"üìÑ CONTENT BRIEF: {page}")
    print(f"{'='*80}")
    print(f"Total Keywords: {brief['total_keywords']}")
    print(f"Total Volume: {brief['total_volume']:,.0f}")
    print(f"Avg Match Score: {brief['avg_match_score']:.2%}")

    print(f"\nüéØ PRIMARY Keywords:")
    for kw in brief['primary']:
        print(f"   ‚Ä¢ {kw['keyword']} (vol: {kw['volume']:.0f}, match: {kw['best_match_score']:.2%})")

    print(f"\nüéØ SECONDARY Keywords:")
    for kw in brief['secondary']:
        print(f"   ‚Ä¢ {kw['keyword']} (vol: {kw['volume']:.0f}, match: {kw['best_match_score']:.2%})")

    if len(brief['tertiary']) > 0:
        print(f"\nüéØ TERTIARY Keywords (showing first 10):")
        for kw in brief['tertiary'][:10]:
            print(f"   ‚Ä¢ {kw['keyword']} (vol: {kw['volume']:.0f}, match: {kw['best_match_score']:.2%})")

# Export all briefs to CSV for easy copy-paste
all_briefs_data = []
for page, brief in briefs.items():
    for tier, keywords in [('primary', brief['primary']),
                           ('secondary', brief['secondary']),
                           ('tertiary', brief['tertiary'])]:
        for kw in keywords:
            all_briefs_data.append({
                'page': page,
                'tier': tier,
                'keyword': kw['keyword'],
                'volume': kw['volume'],
                'match_score': kw['best_match_score']
            })

briefs_df = pd.DataFrame(all_briefs_data)
briefs_df.to_csv('content_briefs_all.csv', index=False)
print(f"\n‚úÖ Exported content_briefs_all.csv with all keyword assignments")

# ============================================================================
# STEP 13.5A: BONUS - CUSTOM BLURB-BASED SEMANTIC TARGETING (For Difficult Pages)
# ============================================================================

print("\n" + "="*80)
print("üéØ CUSTOM BLURB KEYWORD MATCHING")
print("="*80)

def match_keywords_to_custom_blurb(blurb_text, blurb_name, keywords_df, top_n=50):
    """Match keywords to a custom text blurb that defines your page concept"""

    print(f"\nüìù Matching keywords to: '{blurb_name}'")
    print(f"Blurb length: {len(blurb_text)} characters")
    print(f"\nFirst 150 chars of blurb:\n{blurb_text[:150]}...\n")

    # Generate embedding for your custom blurb
    print("üîÑ Generating blurb embedding...")
    blurb_embedding = get_embedding(blurb_text)

    # Calculate similarity between all keywords and this blurb
    print("üîÑ Calculating keyword similarities...")
    keyword_embeddings = np.array(keywords_df['embedding'].tolist())
    similarities = cosine_similarity(keyword_embeddings, [blurb_embedding]).flatten()

    # Create CLEAN results dataframe
    results_df = pd.DataFrame({
        'keyword': keywords_df['keyword'],
        'volume': keywords_df['volume'],
        'recommendation': '',
        'target': blurb_name,
        'similarity': similarities
    })

    # Add tier recommendations
    def assign_tier(similarity):
        if similarity >= 0.70:
            return 'Primary'
        elif similarity >= 0.65:
            return 'Secondary'
        elif similarity >= 0.60:
            return 'Tertiary'
        else:
            return 'Defer'

    results_df['recommendation'] = results_df['similarity'].apply(assign_tier)

    # Sort by similarity
    results_df = results_df.sort_values('similarity', ascending=False)

    # Reorder columns for easy copy-paste
    results_df = results_df[['keyword', 'volume', 'recommendation', 'target', 'similarity']]

    return results_df.head(top_n)

# ============================================================================
# GOLF/SPORTS EXAMPLE
# ============================================================================

print("\n" + "="*80)
print("EXAMPLE: Golf/Sports Vacation Packages")
print("="*80)

golf_sports_blurb = """Wine tours and wine tour experiences. Winery tours and winery tour packages.Food tours and food tour experiences. Food travel and culinary tours. Wine tasting tours at vineyards. Cooking vacations and culinary vacations. Winery bike tours. Culinary travel and foodie travel. Food tour packages. Regional food trips and food travel destinations. Foodie destinations and best places to travel for food. Wine and food tours. Culinary tour experiences."""

# Match keywords to this custom concept
golf_sports_results = match_keywords_to_custom_blurb(
    blurb_text=golf_sports_blurb,
    blurb_name="Culinary Vacation Packages",
    keywords_df=df_filtered,
    top_n=50
)

print("\nüìä TOP 20 KEYWORD MATCHES:")
print("="*80)
for idx, row in golf_sports_results.head(20).iterrows():
    print(f"   {row['similarity']:.3f} | {row['keyword']} (vol: {row['volume']:.0f}) [{row['recommendation']}]")

print(f"\nüìä TIER DISTRIBUTION:")
print(golf_sports_results['recommendation'].value_counts().to_string())

print(f"\nAverage similarity (top 20): {golf_sports_results.head(20)['similarity'].mean():.3f}")
print(f"Total volume (top 20): {golf_sports_results.head(20)['volume'].sum():.0f}")

# Export
filename = 'custom_blurb_golf_sports.csv'
golf_sports_results.to_csv(filename, index=False)
print(f"\n‚úÖ Exported {filename}")

# ============================================================================
# STEP 13.5B: BONUS - ENTITY-BASED SEMANTIC TARGETING (For Difficult Pages)
# ============================================================================

print("\n" + "="*80)
print("üéØ ENTITY-BASED KEYWORD MATCHING")
print("="*80)

import requests
from bs4 import BeautifulSoup

def fetch_wikipedia_content(page_title):
    """Fetch Wikipedia page content for semantic matching"""
    # Use Wikipedia API with proper headers
    url = "https://en.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "format": "json",
        "titles": page_title,
        "prop": "extracts",
        "explaintext": True,
        "exintro": True,  # Just get the intro section
    }

    # Wikipedia requires a User-Agent
    headers = {
        "User-Agent": "SEOToolBot/1.0 (Educational Purpose; Python/Requests)"
    }

    try:
        response = requests.get(url, params=params, headers=headers, timeout=10)
        response.raise_for_status()  # Raise error for bad status codes

        data = response.json()

        pages = data.get("query", {}).get("pages", {})
        page_id = list(pages.keys())[0]

        if page_id == "-1":
            print(f"   ‚ö†Ô∏è  Page '{page_title}' not found on Wikipedia")
            return None

        content = pages[page_id].get("extract", "")

        if not content:
            print(f"   ‚ö†Ô∏è  Page '{page_title}' has no content")
            return None

        return content

    except requests.exceptions.JSONDecodeError as e:
        print(f"   ‚ùå JSON decode error: {e}")
        print(f"   Response text: {response.text[:200]}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"   ‚ùå Request error: {e}")
        return None
    except Exception as e:
        print(f"   ‚ùå Unexpected error: {e}")
        return None

def match_keywords_to_entity(wikipedia_title, keywords_df, top_n=50):
    """Match keywords to a Wikipedia entity concept"""

    # Fetch Wikipedia content
    print(f"\nüîç Fetching Wikipedia page: '{wikipedia_title}'...")
    wiki_content = fetch_wikipedia_content(wikipedia_title)

    if not wiki_content:
        print(f"‚ùå Could not find Wikipedia page: '{wikipedia_title}'")
        return None

    print(f"‚úÖ Found page ({len(wiki_content)} characters)")
    print(f"\nFirst 200 chars:\n{wiki_content[:200]}...\n")

    # Generate embedding for Wikipedia content
    print("üîÑ Generating entity embedding...")
    entity_embedding = get_embedding(wiki_content)

    # Calculate similarity between all keywords and this entity
    print("üîÑ Calculating keyword similarities...")
    keyword_embeddings = np.array(keywords_df['embedding'].tolist())
    similarities = cosine_similarity(keyword_embeddings, [entity_embedding]).flatten()

    # Create CLEAN results dataframe with only relevant columns
    results_df = pd.DataFrame({
        'keyword': keywords_df['keyword'],
        'volume': keywords_df['volume'],
        'recommendation': '',  # Will be filled in next step
        'entity': wikipedia_title,
        'similarity': similarities
    })

  # Add tier recommendations based on similarity scores
    def assign_tier(similarity):
        if similarity >= 0.70:
            return 'Primary'
        elif similarity >= 0.65:
            return 'Secondary'
        elif similarity >= 0.60:
            return 'Tertiary'
        else:
            return 'Defer'

    results_df['recommendation'] = results_df['similarity'].apply(assign_tier)

    # Sort by similarity (descending)
    results_df = results_df.sort_values('similarity', ascending=False)

    # Reorder columns for easy copy-paste
    results_df = results_df[['keyword', 'volume', 'recommendation', 'entity', 'similarity']]

    return results_df.head(top_n)

# ============================================================================
# WIKIPEDIA CANDIDATES
# ============================================================================

print("\nTesting multiple Wikipedia entities:\n")

# Your three candidate entities
candidate_entities = [
    "Luxury train",
]

corporate_results = {}

for entity in candidate_entities:
    results = match_keywords_to_entity(entity, df_filtered, top_n=30)

    if results is not None:
        corporate_results[entity] = results

        print(f"\n{'='*80}")
        print(f"üìä TOP MATCHES FOR: {entity}")
        print(f"{'='*80}")

        print(f"\nTop 15 keywords:")
        for idx, row in results.head(15).iterrows():
            print(f"   {row['similarity']:.3f} | {row['keyword']} (vol: {row['volume']:.0f})")

        print(f"\nAverage similarity (top 15): {results.head(15)['similarity'].mean():.3f}")

# Compare which entity is the best fit
print("\n" + "="*80)
print("üèÜ ENTITY COMPARISON")
print("="*80)

for entity, results in corporate_results.items():
    avg_sim = results.head(15)['similarity'].mean()  # Changed from 'entity_similarity'
    avg_vol = results.head(15)['volume'].mean()
    print(f"\n{entity}:")
    print(f"   Avg similarity: {avg_sim:.3f}")
    print(f"   Avg volume: {avg_vol:.0f}")
    print(f"   Best match: '{results.iloc[0]['keyword']}' ({results.iloc[0]['similarity']:.3f})")  # Changed here too


# Export best matching entity results
best_entity = max(corporate_results.keys(),
                  key=lambda e: corporate_results[e].head(15)['similarity'].mean())  # Changed from 'entity_similarity'

print(f"\n‚úÖ Best semantic match: {best_entity}")

# Export with clean filename
filename = f'entity_match_{best_entity.replace(" ", "_").lower()}.csv'
corporate_results[best_entity].to_csv(filename, index=False)
print(f"‚úÖ Exported {filename}")

# Show summary of tier distribution
best_results = corporate_results[best_entity]
print(f"\nüìä Tier Distribution for '{best_entity}':")
print(best_results['recommendation'].value_counts().to_string())

# ============================================================================
# STEP 14: ADJUSTMENTS & TIPS
# ============================================================================

print("\n" + "="*80)
print("üîß ADJUSTMENT OPTIONS")
print("="*80)

print("""
If results aren't quite right, you can adjust these parameters:

1. SIMILARITY_THRESHOLD (currently {threshold})
   - Higher (0.70-0.75): Stricter matching, fewer keywords matched per page
   - Lower (0.55-0.60): Looser matching, more keywords matched per page
   - Adjust in STEP 11 and re-run from there

2. PAGE_PROFILES (in STEP 10)
   - Add more specific keywords to better define each page's semantic target
   - Remove generic terms that might cause unwanted matches
   - Example: If 'things-to-do/couples' is matching too many family keywords,
     add more explicitly romantic terms to its profile

3. Review the exported CSVs:
   - matched_keywords.csv: All auto-assigned keywords
   - unmatched_keywords.csv: Keywords below threshold (might need manual review)
   - ambiguous_keywords.csv: Keywords that could go to multiple pages
   - content_briefs_all.csv: Final briefs ready to use

4. High-risk pairs to watch:
   - things-to-do/golf-sports vs vacation-packages/golf-vacations-sports
   - things-to-do/wine-food vs vacation-packages/food-wine
   - things-to-do/family vs vacation-packages/family
   - things-to-do/luxury vs vacation-packages/luxury
   - things-to-do/train vs vacation-packages/train-travel
   - things-to-do/pet-friendly vs vacation-packages/pet-friendly-travel

   Check that informational keywords went to things-to-do and commercial went to packages.

üí° NEXT STEPS:
1. Download content_briefs_all.csv
2. Review keyword assignments (especially for high-risk pairs)
3. Manually adjust any mismatched keywords
4. Copy keywords into your actual content brief templates
5. Start writing! üéâ
""".format(threshold=SIMILARITY_THRESHOLD))

print("\n" + "="*80)
print("‚úÖ COMPLETE! Your content briefs are ready.")
print("="*80)
